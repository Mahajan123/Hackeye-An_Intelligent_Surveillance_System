{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os, argparse\n",
    "import cv2, spacy, numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.externals import joblib\n",
    "from keras import backend as K\n",
    "#import keras.backend.tensorflow_backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_model(CNN_weights_file_name):\n",
    "    ''' Takes the CNN weights file, and returns the VGG model update \n",
    "    with the weights. Requires the file VGG.py inside models/CNN '''\n",
    "    from VGG import VGG_16\n",
    "    image_model = VGG_16(CNN_weights_file_name)\n",
    "\n",
    "    # this is standard VGG 16 without the last two layers\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # one may experiment with \"adam\" optimizer, but the loss function for\n",
    "    # this kind of task is pretty standard\n",
    "    image_model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    return image_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(image_file_name):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the \n",
    "    weights (filters) as a 1, 4096 dimension vector '''\n",
    "    image_features = np.zeros((1, 4096))\n",
    "    # Magic_Number = 4096  > Comes from last layer of VGG Model\n",
    "\n",
    "    # Since VGG was trained as a image of 224x224, every new image\n",
    "    # is required to go through the same transformation\n",
    "    im = cv2.resize(cv2.imread(image_file_name), (224, 224))\n",
    "    im = im.transpose((2,0,1)) # convert the image to RGBA\n",
    "\n",
    "    \n",
    "    # this axis dimension is required because VGG was trained on a dimension\n",
    "    # of 1, 3, 224, 224 (first axis is for the batch size\n",
    "    # even though we are using only one image, we have to keep the dimensions consistent\n",
    "    im = np.expand_dims(im, axis=0) \n",
    "\n",
    "    image_features[0,:] = model_vgg.predict(im)[0]\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_features(question):\n",
    "    ''' For a given question, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    #     print(word_embeddings)\n",
    "    tokens = word_embeddings(question)\n",
    "    token = np.asarray(tokens)\n",
    "#     print(token)\n",
    "    question_tensor = np.zeros((1, 30, 300))\n",
    "    for j in range(len(tokens)):\n",
    "#         print(len(tokens[j].vector))\n",
    "        question_tensor[0,j,:] = tokens[j].vector[:300]\n",
    "        \n",
    "    return question_tensor\n",
    "# print(get_question_features(\"how are you\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VQA_model(VQA_weights_file_name):\n",
    "    #Given the VQA model and its weights, compiles and returns the model \n",
    "\n",
    "    from VQA import VQA_MODEL\n",
    "    vqa_model = VQA_MODEL()\n",
    "    vqa_model.load_weights(VQA_weights_file_name)\n",
    "\n",
    "    vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return vqa_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract frames \n",
    "def FrameCapture(path): \n",
    "      \n",
    "    # Path to video file \n",
    "    vidObj = cv2.VideoCapture(path) \n",
    "  \n",
    "    # Used as counter variable \n",
    "    count = 0\n",
    "  \n",
    "    # checks whether frames were extracted \n",
    "    success = 1\n",
    "  \n",
    "    while success: \n",
    "  \n",
    "        # vidObj object calls read \n",
    "        # function extract frames \n",
    "        success, image = vidObj.read() \n",
    "  \n",
    "        # Saves the frames with frame-count \n",
    "        cv2.imwrite(\"frame%d.jpg\" % count, image) \n",
    "  \n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2683: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2550: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "417\n",
      "['what is in the picture', 'what is color of picture']\n",
      "image no= 0\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame0.jpg\n",
      "what is in the picture\n",
      "output of image = 50\n",
      "88\n",
      "47.44 %  bananas\n",
      "375\n",
      "14.18 %  fruit\n",
      "660\n",
      "06.57 %  plants\n",
      "126\n",
      "04.56 %  birds\n",
      "314\n",
      "03.46 %  ducks\n",
      "image no= 50\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame50.jpg\n",
      "what is in the picture\n",
      "output of image = 100\n",
      "88\n",
      "73.29 %  bananas\n",
      "375\n",
      "10.38 %  fruit\n",
      "347\n",
      "01.93 %  fish\n",
      "660\n",
      "01.88 %  plants\n",
      "360\n",
      "001.0 %  food\n",
      "image no= 100\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame100.jpg\n",
      "what is in the picture\n",
      "output of image = 150\n",
      "88\n",
      "35.52 %  bananas\n",
      "375\n",
      "017.6 %  fruit\n",
      "660\n",
      "006.4 %  plants\n",
      "360\n",
      "04.18 %  food\n",
      "347\n",
      "003.3 %  fish\n",
      "image no= 150\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame150.jpg\n",
      "what is in the picture\n",
      "output of image = 200\n",
      "375\n",
      "40.74 %  fruit\n",
      "88\n",
      "35.74 %  bananas\n",
      "660\n",
      "03.01 %  plants\n",
      "126\n",
      "02.89 %  birds\n",
      "360\n",
      "02.22 %  food\n",
      "image no= 200\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame200.jpg\n",
      "what is in the picture\n",
      "output of image = 250\n",
      "88\n",
      "019.2 %  bananas\n",
      "375\n",
      "15.88 %  fruit\n",
      "360\n",
      "006.5 %  food\n",
      "474\n",
      "06.43 %  kite\n",
      "347\n",
      "05.47 %  fish\n",
      "image no= 250\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame250.jpg\n",
      "what is in the picture\n",
      "output of image = 300\n",
      "126\n",
      "22.36 %  birds\n",
      "658\n",
      "08.62 %  plane\n",
      "88\n",
      "08.58 %  bananas\n",
      "375\n",
      "08.53 %  fruit\n",
      "474\n",
      "08.31 %  kite\n",
      "image no= 300\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame300.jpg\n",
      "what is in the picture\n",
      "output of image = 350\n",
      "126\n",
      "21.82 %  birds\n",
      "474\n",
      "18.03 %  kite\n",
      "125\n",
      "13.19 %  bird\n",
      "658\n",
      "06.05 %  plane\n",
      "439\n",
      "03.44 %  horse\n",
      "image no= 350\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame350.jpg\n",
      "what is in the picture\n",
      "output of image = 400\n",
      "474\n",
      "46.71 %  kite\n",
      "658\n",
      "11.45 %  plane\n",
      "58\n",
      "11.29 %  airplane\n",
      "126\n",
      "007.1 %  birds\n",
      "125\n",
      "05.69 %  bird\n",
      "image no= 400\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame400.jpg\n",
      "what is in the picture\n",
      "output of image = 450\n",
      "474\n",
      "69.42 %  kite\n",
      "658\n",
      "05.52 %  plane\n",
      "126\n",
      "04.44 %  birds\n",
      "125\n",
      "04.26 %  bird\n",
      "347\n",
      "03.41 %  fish\n",
      "image no= 0\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame0.jpg\n",
      "what is color of picture\n",
      "output of image = 50\n",
      "128\n",
      "047.6 %  black\n",
      "960\n",
      "45.62 %  white\n",
      "136\n",
      "01.67 %  blue\n",
      "404\n",
      "01.62 %  gray\n",
      "769\n",
      "01.26 %  silver\n",
      "image no= 50\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame50.jpg\n",
      "what is color of picture\n",
      "output of image = 100\n",
      "960\n",
      "76.33 %  white\n",
      "128\n",
      "14.62 %  black\n",
      "130\n",
      "02.23 %  black and white\n",
      "136\n",
      "01.82 %  blue\n",
      "769\n",
      "01.56 %  silver\n",
      "image no= 100\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame100.jpg\n",
      "what is color of picture\n",
      "output of image = 150\n",
      "960\n",
      "057.8 %  white\n",
      "128\n",
      "35.26 %  black\n",
      "130\n",
      "02.25 %  black and white\n",
      "990\n",
      "00.98 %  yellow\n",
      "136\n",
      "00.78 %  blue\n",
      "image no= 150\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame150.jpg\n",
      "what is color of picture\n",
      "output of image = 200\n",
      "960\n",
      "57.76 %  white\n",
      "128\n",
      "34.43 %  black\n",
      "130\n",
      "01.94 %  black and white\n",
      "769\n",
      "01.43 %  silver\n",
      "404\n",
      "01.14 %  gray\n",
      "image no= 200\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame200.jpg\n",
      "what is color of picture\n",
      "output of image = 250\n",
      "960\n",
      "078.0 %  white\n",
      "128\n",
      "015.4 %  black\n",
      "130\n",
      "03.76 %  black and white\n",
      "769\n",
      "00.78 %  silver\n",
      "136\n",
      "00.53 %  blue\n",
      "image no= 250\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame250.jpg\n",
      "what is color of picture\n",
      "output of image = 300\n",
      "960\n",
      "58.69 %  white\n",
      "128\n",
      "36.75 %  black\n",
      "130\n",
      "01.69 %  black and white\n",
      "404\n",
      "01.07 %  gray\n",
      "136\n",
      "000.5 %  blue\n",
      "image no= 300\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame300.jpg\n",
      "what is color of picture\n",
      "output of image = 350\n",
      "960\n",
      "087.8 %  white\n",
      "128\n",
      "10.57 %  black\n",
      "130\n",
      "00.82 %  black and white\n",
      "404\n",
      "00.24 %  gray\n",
      "769\n",
      "000.2 %  silver\n",
      "image no= 350\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame350.jpg\n",
      "what is color of picture\n",
      "output of image = 400\n",
      "960\n",
      "073.6 %  white\n",
      "128\n",
      "024.1 %  black\n",
      "130\n",
      "00.87 %  black and white\n",
      "404\n",
      "00.41 %  gray\n",
      "136\n",
      "00.28 %  blue\n",
      "image no= 400\n",
      "/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame400.jpg\n",
      "what is color of picture\n",
      "output of image = 450\n",
      "960\n",
      "84.73 %  white\n",
      "128\n",
      "12.75 %  black\n",
      "130\n",
      "01.37 %  black and white\n",
      "404\n",
      "00.35 %  gray\n",
      "136\n",
      "000.2 %  blue\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Driver Code \n",
    "if __name__ == '__main__': \n",
    "  \n",
    "    VQA_model_file_name      = 'VQA_MODEL.json'\n",
    "    VQA_weights_file_name   = 'VQA_MODEL_WEIGHTS.hdf5'\n",
    "    label_encoder_file_name  = 'FULL_labelencoder_trainval.pkl'\n",
    "    CNN_weights_file_name   = 'vgg16_weights.h5'\n",
    "    \n",
    "    model_vgg = get_image_model(CNN_weights_file_name)\n",
    "    \n",
    "    word_embeddings = spacy.load('en_vectors_web_lg')\n",
    "    \n",
    "    model_vqa = get_VQA_model(VQA_weights_file_name)\n",
    "\n",
    "    a=FrameCapture(\"/home/priya/Downloads/COLLEGE/PROJECT/framecode/aa2.mp4\") \n",
    "    print(a)\n",
    "    \n",
    "    with open(\"question.txt\") as f:\n",
    "        content = f.read().splitlines()\n",
    "        print(content)\n",
    "    \n",
    "    length = len(content)\n",
    "\n",
    "    for i in range(length):\n",
    "\n",
    "        cnt=0\n",
    "        while(cnt<=a):\n",
    "            print('image no=',cnt)\n",
    "            strvar=str(cnt)\n",
    "            #print(type(strvar))\n",
    "            #print(strvar)\n",
    "            path='/home/priya/Downloads/COLLEGE/PROJECT/framecode/frame'\n",
    "            image_file_name=path+strvar+'.jpg'\n",
    "            #print(type(image_file_name))\n",
    "            print(image_file_name)\n",
    "            cnt=cnt+50\n",
    "            #question = \"what is in the picture\"\n",
    "            question = content[i]\n",
    "            print(question)\n",
    "            image_features = get_image_features(image_file_name)\n",
    "            question_features = get_question_features((question))\n",
    "            y_output = model_vqa.predict([question_features, image_features])\n",
    "            warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "            labelencoder = joblib.load(label_encoder_file_name)\n",
    "\n",
    "            print('output of image =',cnt)\n",
    "            for label in reversed(np.argsort(y_output)[0,-5:]):\n",
    "                print(label)\n",
    "                print (str(round(y_output[0,label]*100,2)).zfill(5), \"% \", labelencoder.inverse_transform(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
